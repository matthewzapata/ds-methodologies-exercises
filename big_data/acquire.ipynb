{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using case.csv & dept.csv:\n",
    "\n",
    "1. read into spark environment (df_case, df_dept)\n",
    "1. write df_case and df_dept back to disk into their own directories (my_cases and my_depts)\n",
    "1. Write df_case and df_dept to parquet files (my_cases_parquet and my_depts_parquet)\n",
    "1. Read your parquet files back into your spark environment.\n",
    "1. Read case.csv and dept.csv into a pandas dataframe. (cases_pdf, depts_pdf)\n",
    "1. Convert the pandas dataframes into spark dataframes (cases_sdf, depts_sdf)\n",
    "1. Convert the spark dataframes back into pandas dataframes. (cases_pdf1, depts_pdf1)\n",
    "1. Write the spark dataframes (cases_sdf, depts_sdf) to Hive tables.\n",
    "1. Explore the Hive database/tables you have created using the methods in the lesson.\n",
    "1. Read from the tables into two spark dataframes (cases_sdf, depts_sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string, _c9: string, _c10: string, _c11: string, _c12: string, _c13: string]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.csv('./sa311/case.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string, _c9: string, _c10: string, _c11: string, _c12: string, _c13: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format('csv').load('./sa311/case.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------+----------------------+-------------------+\n",
      "|  dept_division|       dept_name|standardized_dept_name|dept_subject_to_SLA|\n",
      "+---------------+----------------+----------------------+-------------------+\n",
      "|311 Call Center|Customer Service|      Customer Service|                YES|\n",
      "+---------------+----------------+----------------------+-------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dept = (spark.read\n",
    "           .option('header', True)\n",
    "           .option('inferSchema', True)\n",
    "           .format('csv')\n",
    "           .load('./sa311/dept.csv'))\n",
    "\n",
    "df_dept.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+----------------+------------+---------+------------------+-----------+----------------+--------------------+--------+-----------+---------+--------------------+----------------+\n",
      "|   case_id|case_opened_date|case_closed_date|SLA_due_date|case_late|     num_days_late|case_closed|   dept_division|service_request_type|SLA_days|case_status|source_id|     request_address|council_district|\n",
      "+----------+----------------+----------------+------------+---------+------------------+-----------+----------------+--------------------+--------+-----------+---------+--------------------+----------------+\n",
      "|1014127332|     1/1/18 0:42|    1/1/18 12:29|9/26/20 0:42|       NO|-998.5087616000001|        YES|Field Operations|        Stray Animal|   999.0|     Closed| svcCRMLS|2315  EL PASO ST,...|               5|\n",
      "+----------+----------------+----------------+------------+---------+------------------+-----------+----------------+--------------------+--------+-----------+---------+--------------------+----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_case = (spark.read\n",
    " .option('header', True)\n",
    " .option('inferSchema', True)\n",
    " .format('csv')\n",
    " .load('./sa311/case.csv'))\n",
    "\n",
    "df_case.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dept_division', 'string'),\n",
       " ('dept_name', 'string'),\n",
       " ('standardized_dept_name', 'string'),\n",
       " ('dept_subject_to_SLA', 'string')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dept.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('case_id', 'int'),\n",
       " ('case_opened_date', 'string'),\n",
       " ('case_closed_date', 'string'),\n",
       " ('SLA_due_date', 'string'),\n",
       " ('case_late', 'string'),\n",
       " ('num_days_late', 'double'),\n",
       " ('case_closed', 'string'),\n",
       " ('dept_division', 'string'),\n",
       " ('service_request_type', 'string'),\n",
       " ('SLA_days', 'double'),\n",
       " ('case_status', 'string'),\n",
       " ('source_id', 'string'),\n",
       " ('request_address', 'string'),\n",
       " ('council_district', 'int')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_case.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving as csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_case.write\n",
    " .format('csv')\n",
    " .mode('overwrite')\n",
    " .option('header', True)\n",
    " .save('my_cases'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+----------------+-------------+---------+-------------+-----------+----------------+--------------------+-----------+-----------+---------+--------------------+----------------+\n",
      "|   case_id|case_opened_date|case_closed_date| SLA_due_date|case_late|num_days_late|case_closed|   dept_division|service_request_type|   SLA_days|case_status|source_id|     request_address|council_district|\n",
      "+----------+----------------+----------------+-------------+---------+-------------+-----------+----------------+--------------------+-----------+-----------+---------+--------------------+----------------+\n",
      "|1014551581|   5/28/18 13:14|   5/28/18 14:23|5/28/18 16:14|       NO| -0.077511574|        YES|Field Operations|     Officer Standby|      0.125|     Closed|  NO10960|7003  RAVENSDALE,...|               6|\n",
      "|1014551583|   5/28/18 13:15|   5/29/18 14:38|  6/1/18 8:30|       NO| -2.743912037|        YES|Waste Collection|           No Pickup|   3.801875|     Closed|   138793|1906  MOSSY CREEK...|               4|\n",
      "|1014551584|   5/28/18 13:16|   5/29/18 14:31|  6/5/18 8:30|       NO| -6.749131944|        YES|Waste Collection|    Lost/Stolen Cart|7.801087963|     Closed|   142989|103  SPRINGWOOD L...|               1|\n",
      "+----------+----------------+----------------+-------------+---------+-------------+-----------+----------------+--------------------+-----------+-----------+---------+--------------------+----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.option('header', True).csv('my_cases').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dept.write.format('csv').mode(\"overwrite\").\\\n",
    "    option(\"header\", True).save(\"my_depts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+----------------------+-------------------+\n",
      "|  dept_division|           dept_name|standardized_dept_name|dept_subject_to_SLA|\n",
      "+---------------+--------------------+----------------------+-------------------+\n",
      "|311 Call Center|    Customer Service|      Customer Service|                YES|\n",
      "|          Brush|Solid Waste Manag...|           Solid Waste|                YES|\n",
      "|Clean and Green|Parks and Recreation|    Parks & Recreation|                YES|\n",
      "+---------------+--------------------+----------------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.option('header', True).csv('my_depts').show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving as parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_case.write.format('parquet').mode('overwrite').option('header', True).save('my_cases_parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dept.write.format('parquet').mode('overwrite').option('header', True).save('my_depts_parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+----------------+-------------+---------+-------------+-----------+----------------+--------------------+-----------+-----------+---------+--------------------+----------------+\n",
      "|   case_id|case_opened_date|case_closed_date| SLA_due_date|case_late|num_days_late|case_closed|   dept_division|service_request_type|   SLA_days|case_status|source_id|     request_address|council_district|\n",
      "+----------+----------------+----------------+-------------+---------+-------------+-----------+----------------+--------------------+-----------+-----------+---------+--------------------+----------------+\n",
      "|1014551581|   5/28/18 13:14|   5/28/18 14:23|5/28/18 16:14|       NO| -0.077511574|        YES|Field Operations|     Officer Standby|      0.125|     Closed|  NO10960|7003  RAVENSDALE,...|               6|\n",
      "|1014551583|   5/28/18 13:15|   5/29/18 14:38|  6/1/18 8:30|       NO| -2.743912037|        YES|Waste Collection|           No Pickup|   3.801875|     Closed|   138793|1906  MOSSY CREEK...|               4|\n",
      "|1014551584|   5/28/18 13:16|   5/29/18 14:31|  6/5/18 8:30|       NO| -6.749131944|        YES|Waste Collection|    Lost/Stolen Cart|7.801087963|     Closed|   142989|103  SPRINGWOOD L...|               1|\n",
      "+----------+----------------+----------------+-------------+---------+-------------+-----------+----------------+--------------------+-----------+-----------+---------+--------------------+----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.option('header', True).parquet('my_cases_parquet').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+----------------------+-------------------+\n",
      "|  dept_division|           dept_name|standardized_dept_name|dept_subject_to_SLA|\n",
      "+---------------+--------------------+----------------------+-------------------+\n",
      "|311 Call Center|    Customer Service|      Customer Service|                YES|\n",
      "|          Brush|Solid Waste Manag...|           Solid Waste|                YES|\n",
      "|Clean and Green|Parks and Recreation|    Parks & Recreation|                YES|\n",
      "+---------------+--------------------+----------------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.option('header', True).parquet('my_depts_parquet').show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read each csv into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_pdf = pd.read_csv('./sa311/case.csv')\n",
    "depts_pdf = pd.read_csv('./sa311/dept.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the pandas dataframes into spark dataframes\n",
    "Fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+----------------+------------+---------+-------------+-----------+----------------+--------------------+-----------+-----------+---------+--------------------+----------------+\n",
      "|   case_id|case_opened_date|case_closed_date|SLA_due_date|case_late|num_days_late|case_closed|   dept_division|service_request_type|   SLA_days|case_status|source_id|     request_address|council_district|\n",
      "+----------+----------------+----------------+------------+---------+-------------+-----------+----------------+--------------------+-----------+-----------+---------+--------------------+----------------+\n",
      "|1014127332|     1/1/18 0:42|    1/1/18 12:29|9/26/20 0:42|       NO| -998.5087616|        YES|Field Operations|        Stray Animal|      999.0|     Closed| svcCRMLS|2315  EL PASO ST,...|               5|\n",
      "|1014127333|     1/1/18 0:46|     1/3/18 8:11| 1/5/18 8:30|       NO| -2.012604167|        YES|     Storm Water|Removal Of Obstru...|4.322222222|     Closed| svcCRMSS|2215  GOLIAD RD, ...|               3|\n",
      "|1014127334|     1/1/18 0:48|     1/2/18 7:57| 1/5/18 8:30|       NO| -3.022337963|        YES|     Storm Water|Removal Of Obstru...|4.320729167|     Closed| svcCRMSS|102  PALFREY ST W...|               3|\n",
      "|1014127335|     1/1/18 1:29|     1/2/18 8:13|1/17/18 8:30|       NO| -15.01148148|        YES|Code Enforcement|Front Or Side Yar...|16.29188657|     Closed| svcCRMSS|114  LA GARDE ST,...|               3|\n",
      "|1014127336|     1/1/18 1:34|    1/1/18 13:29| 1/1/18 4:34|      YES|  0.372164352|        YES|Field Operations|Animal Cruelty(Cr...|      0.125|     Closed| svcCRMSS|734  CLEARVIEW DR...|               7|\n",
      "+----------+----------------+----------------+------------+---------+-------------+-----------+----------------+--------------------+-----------+-----------+---------+--------------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.types as T\n",
    "schema = T.StructType([T.StructField('case_id', T.StringType()),\n",
    "                      T.StructField('case_opened_date', T.StringType()),\n",
    "                      T.StructField('case_closed_date', T.StringType()),\n",
    "                      T.StructField('SLA_due_date', T.StringType()),\n",
    "                      T.StructField('case_late', T.StringType()),\n",
    "                      T.StructField('num_days_late', T.StringType()),\n",
    "                      T.StructField('case_closed', T.StringType()),\n",
    "                      T.StructField('dept_division', T.StringType()),\n",
    "                      T.StructField('service_request_type', T.StringType()),\n",
    "                      T.StructField('SLA_days', T.StringType()),\n",
    "                      T.StructField('case_status', T.StringType()),\n",
    "                      T.StructField('source_id', T.StringType()),\n",
    "                      T.StructField('request_address', T.StringType()),\n",
    "                      T.StructField('council_district', T.StringType())])\n",
    "\n",
    "spark.createDataFrame(cases_pdf, schema).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------------------+-------------------+\n",
      "|       dept_division|           dept_name|standardized_dept_name|dept_subject_to_SLA|\n",
      "+--------------------+--------------------+----------------------+-------------------+\n",
      "|     311 Call Center|    Customer Service|      Customer Service|                YES|\n",
      "|               Brush|Solid Waste Manag...|           Solid Waste|                YES|\n",
      "|     Clean and Green|Parks and Recreation|    Parks & Recreation|                YES|\n",
      "|Clean and Green N...|Parks and Recreation|    Parks & Recreation|                YES|\n",
      "|    Code Enforcement|Code Enforcement ...|  DSD/Code Enforcement|                YES|\n",
      "+--------------------+--------------------+----------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = T.StructType([T.StructField('dept_division', T.StringType()),\n",
    "                      T.StructField('dept_name', T.StringType()),\n",
    "                      T.StructField('standardized_dept_name', T.StringType()),\n",
    "                      T.StructField('dept_subject_to_SLA', T.StringType())])\n",
    "\n",
    "spark.createDataFrame(depts_pdf, schema).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the spark dataframes back into pandas dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_sdf = spark.read.option('header', True).csv('my_cases')\n",
    "depts_sdf = spark.read.option('header', True).csv('my_depts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_pdf1 = cases_sdf.toPandas()\n",
    "depts_pdf1 = depts_sdf.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>case_opened_date</th>\n",
       "      <th>case_closed_date</th>\n",
       "      <th>SLA_due_date</th>\n",
       "      <th>case_late</th>\n",
       "      <th>num_days_late</th>\n",
       "      <th>case_closed</th>\n",
       "      <th>dept_division</th>\n",
       "      <th>service_request_type</th>\n",
       "      <th>SLA_days</th>\n",
       "      <th>case_status</th>\n",
       "      <th>source_id</th>\n",
       "      <th>request_address</th>\n",
       "      <th>council_district</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1014551581</td>\n",
       "      <td>5/28/18 13:14</td>\n",
       "      <td>5/28/18 14:23</td>\n",
       "      <td>5/28/18 16:14</td>\n",
       "      <td>NO</td>\n",
       "      <td>-0.077511574</td>\n",
       "      <td>YES</td>\n",
       "      <td>Field Operations</td>\n",
       "      <td>Officer Standby</td>\n",
       "      <td>0.125</td>\n",
       "      <td>Closed</td>\n",
       "      <td>NO10960</td>\n",
       "      <td>7003  RAVENSDALE, San Antonio, 78250</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1014551583</td>\n",
       "      <td>5/28/18 13:15</td>\n",
       "      <td>5/29/18 14:38</td>\n",
       "      <td>6/1/18 8:30</td>\n",
       "      <td>NO</td>\n",
       "      <td>-2.743912037</td>\n",
       "      <td>YES</td>\n",
       "      <td>Waste Collection</td>\n",
       "      <td>No Pickup</td>\n",
       "      <td>3.801875</td>\n",
       "      <td>Closed</td>\n",
       "      <td>138793</td>\n",
       "      <td>1906  MOSSY CREEK DR, San Antonio, 78245</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1014551584</td>\n",
       "      <td>5/28/18 13:16</td>\n",
       "      <td>5/29/18 14:31</td>\n",
       "      <td>6/5/18 8:30</td>\n",
       "      <td>NO</td>\n",
       "      <td>-6.749131944</td>\n",
       "      <td>YES</td>\n",
       "      <td>Waste Collection</td>\n",
       "      <td>Lost/Stolen Cart</td>\n",
       "      <td>7.801087963</td>\n",
       "      <td>Closed</td>\n",
       "      <td>142989</td>\n",
       "      <td>103  SPRINGWOOD LN, San Antonio, 78216</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      case_id case_opened_date case_closed_date   SLA_due_date case_late  \\\n",
       "0  1014551581    5/28/18 13:14    5/28/18 14:23  5/28/18 16:14        NO   \n",
       "1  1014551583    5/28/18 13:15    5/29/18 14:38    6/1/18 8:30        NO   \n",
       "2  1014551584    5/28/18 13:16    5/29/18 14:31    6/5/18 8:30        NO   \n",
       "\n",
       "  num_days_late case_closed     dept_division service_request_type  \\\n",
       "0  -0.077511574         YES  Field Operations      Officer Standby   \n",
       "1  -2.743912037         YES  Waste Collection            No Pickup   \n",
       "2  -6.749131944         YES  Waste Collection     Lost/Stolen Cart   \n",
       "\n",
       "      SLA_days case_status source_id  \\\n",
       "0        0.125      Closed   NO10960   \n",
       "1     3.801875      Closed    138793   \n",
       "2  7.801087963      Closed    142989   \n",
       "\n",
       "                            request_address council_district  \n",
       "0      7003  RAVENSDALE, San Antonio, 78250                6  \n",
       "1  1906  MOSSY CREEK DR, San Antonio, 78245                4  \n",
       "2    103  SPRINGWOOD LN, San Antonio, 78216                1  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cases_pdf1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dept_division</th>\n",
       "      <th>dept_name</th>\n",
       "      <th>standardized_dept_name</th>\n",
       "      <th>dept_subject_to_SLA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>311 Call Center</td>\n",
       "      <td>Customer Service</td>\n",
       "      <td>Customer Service</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Brush</td>\n",
       "      <td>Solid Waste Management</td>\n",
       "      <td>Solid Waste</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Clean and Green</td>\n",
       "      <td>Parks and Recreation</td>\n",
       "      <td>Parks &amp; Recreation</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     dept_division               dept_name standardized_dept_name  \\\n",
       "0  311 Call Center        Customer Service       Customer Service   \n",
       "1            Brush  Solid Waste Management            Solid Waste   \n",
       "2  Clean and Green    Parks and Recreation     Parks & Recreation   \n",
       "\n",
       "  dept_subject_to_SLA  \n",
       "0                 YES  \n",
       "1                 YES  \n",
       "2                 YES  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depts_pdf1.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the spark dataframes (cases_sdf, depts_sdf) to Hive tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_sdf.write.saveAsTable('df_cases_hive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+\n",
      "|            col_name|data_type|comment|\n",
      "+--------------------+---------+-------+\n",
      "|             case_id|   string|   null|\n",
      "|    case_opened_date|   string|   null|\n",
      "|    case_closed_date|   string|   null|\n",
      "|        SLA_due_date|   string|   null|\n",
      "|           case_late|   string|   null|\n",
      "|       num_days_late|   string|   null|\n",
      "|         case_closed|   string|   null|\n",
      "|       dept_division|   string|   null|\n",
      "|service_request_type|   string|   null|\n",
      "|            SLA_days|   string|   null|\n",
      "|         case_status|   string|   null|\n",
      "|           source_id|   string|   null|\n",
      "|     request_address|   string|   null|\n",
      "|    council_district|   string|   null|\n",
      "+--------------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('DESCRIBE df_cases_hive').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "depts_sdf.write.saveAsTable('df_depts_hive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+\n",
      "|            col_name|data_type|comment|\n",
      "+--------------------+---------+-------+\n",
      "|       dept_division|   string|   null|\n",
      "|           dept_name|   string|   null|\n",
      "|standardized_dept...|   string|   null|\n",
      "| dept_subject_to_SLA|   string|   null|\n",
      "+--------------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('DESCRIBE df_depts_hive').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the Hive database/tables you have created using the methods in the lesson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+\n",
      "|case_closed_date|case_closed|\n",
      "+----------------+-----------+\n",
      "|   5/28/18 14:23|        YES|\n",
      "|   5/29/18 14:38|        YES|\n",
      "|   5/29/18 14:31|        YES|\n",
      "|   5/29/18 12:13|        YES|\n",
      "|   7/10/18 11:42|        YES|\n",
      "|   5/29/18 14:38|        YES|\n",
      "|   5/31/18 10:15|        YES|\n",
      "|    6/5/18 13:40|        YES|\n",
      "|   5/30/18 13:21|        YES|\n",
      "|     6/5/18 8:37|        YES|\n",
      "|   5/28/18 14:23|        YES|\n",
      "|   5/28/18 14:17|        YES|\n",
      "|   5/29/18 15:43|        YES|\n",
      "|    5/30/18 6:32|        YES|\n",
      "|   5/28/18 15:43|        YES|\n",
      "|    5/30/18 8:31|        YES|\n",
      "|   5/29/18 14:06|        YES|\n",
      "|    5/29/18 9:51|        YES|\n",
      "|   5/28/18 14:13|        YES|\n",
      "|   5/29/18 12:18|        YES|\n",
      "+----------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT case_closed_date, case_closed FROM df_cases_hive').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-----------+\n",
      "|database|    tableName|isTemporary|\n",
      "+--------+-------------+-----------+\n",
      "| default|df_cases_hive|      false|\n",
      "| default|df_depts_hive|      false|\n",
      "+--------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('USE default')\n",
    "spark.sql('SHOW TABLES').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+\n",
      "|            col_name|data_type|comment|\n",
      "+--------------------+---------+-------+\n",
      "|             case_id|   string|   null|\n",
      "|    case_opened_date|   string|   null|\n",
      "|    case_closed_date|   string|   null|\n",
      "|        SLA_due_date|   string|   null|\n",
      "|           case_late|   string|   null|\n",
      "|       num_days_late|   string|   null|\n",
      "|         case_closed|   string|   null|\n",
      "|       dept_division|   string|   null|\n",
      "|service_request_type|   string|   null|\n",
      "|            SLA_days|   string|   null|\n",
      "|         case_status|   string|   null|\n",
      "|           source_id|   string|   null|\n",
      "|     request_address|   string|   null|\n",
      "|    council_district|   string|   null|\n",
      "+--------------------+---------+-------+\n",
      "\n",
      "+--------------------+---------+-------+\n",
      "|            col_name|data_type|comment|\n",
      "+--------------------+---------+-------+\n",
      "|       dept_division|   string|   null|\n",
      "|           dept_name|   string|   null|\n",
      "|standardized_dept...|   string|   null|\n",
      "| dept_subject_to_SLA|   string|   null|\n",
      "+--------------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for table in ['df_cases_hive', 'df_depts_hive']:\n",
    "    spark.sql(f\"DESCRIBE {table}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+----------------+-------------+---------+-------------+-----------+----------------+--------------------+-----------+-----------+---------+--------------------+----------------+\n",
      "|   case_id|case_opened_date|case_closed_date| SLA_due_date|case_late|num_days_late|case_closed|   dept_division|service_request_type|   SLA_days|case_status|source_id|     request_address|council_district|\n",
      "+----------+----------------+----------------+-------------+---------+-------------+-----------+----------------+--------------------+-----------+-----------+---------+--------------------+----------------+\n",
      "|1014551581|   5/28/18 13:14|   5/28/18 14:23|5/28/18 16:14|       NO| -0.077511574|        YES|Field Operations|     Officer Standby|      0.125|     Closed|  NO10960|7003  RAVENSDALE,...|               6|\n",
      "|1014551583|   5/28/18 13:15|   5/29/18 14:38|  6/1/18 8:30|       NO| -2.743912037|        YES|Waste Collection|           No Pickup|   3.801875|     Closed|   138793|1906  MOSSY CREEK...|               4|\n",
      "|1014551584|   5/28/18 13:16|   5/29/18 14:31|  6/5/18 8:30|       NO| -6.749131944|        YES|Waste Collection|    Lost/Stolen Cart|7.801087963|     Closed|   142989|103  SPRINGWOOD L...|               1|\n",
      "+----------+----------------+----------------+-------------+---------+-------------+-----------+----------------+--------------------+-----------+-----------+---------+--------------------+----------------+\n",
      "\n",
      "+---------------+--------------------+----------------------+-------------------+\n",
      "|  dept_division|           dept_name|standardized_dept_name|dept_subject_to_SLA|\n",
      "+---------------+--------------------+----------------------+-------------------+\n",
      "|311 Call Center|    Customer Service|      Customer Service|                YES|\n",
      "|          Brush|Solid Waste Manag...|           Solid Waste|                YES|\n",
      "|Clean and Green|Parks and Recreation|    Parks & Recreation|                YES|\n",
      "+---------------+--------------------+----------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for table in ['df_cases_hive', 'df_depts_hive']:\n",
    "    spark.sql(f\"SELECT * FROM {table} LIMIT 3\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+--------------------+----------------+----------------+\n",
      "|   dept_division|service_request_type|     request_address|case_opened_date|case_closed_date|\n",
      "+----------------+--------------------+--------------------+----------------+----------------+\n",
      "|Waste Collection|           No Pickup|1950  DONALDSON A...|   9/25/17 17:31|    9/28/17 7:30|\n",
      "+----------------+--------------------+--------------------+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(spark\n",
    " .sql('SELECT dept_division, service_request_type, request_address, case_opened_date, case_closed_date \\\n",
    " FROM df_cases_hive WHERE request_address LIKE \"1950  DONALDSON%\"')\n",
    " .show(50)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
